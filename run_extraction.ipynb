{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "648726df-4191-4031-8c74-7cb1e62df89d",
   "metadata": {},
   "source": [
    "# Algorithm\n",
    "\n",
    "Given: Document collection $D$\n",
    "\n",
    "Input: An event $e$\n",
    "\n",
    "1. Find all documents that talk about $e$, we'll denote them as $D_e$\n",
    "    1. Can be done with classical IR or neural methods\n",
    "1. For all $d\\in D_e$ do\n",
    "    1. Extract a sequence of major subevents of $e$ as reported in $d$. We'll call them $e_{d,i}, i\\in \\{0,1,\\dots,n\\}$\n",
    "1. Compare all pairs of subevents for equivalence. Incomplete notation: $(e_{d_k,i}, e_{d_l,j}), k\\neq l$\n",
    "1. Extract connected components of matched pairs: These are all referring to the same subevent!\n",
    "1. Subevent canonicalization (well not really, but ideally!): find the overarching name for each found component!\n",
    "    1. E.g. many subevents go like \"In 2003, the US launched their attack on Iraq following [...]\". These need to be all grouped under an umbrella term, e.g. \"US Invasion of Iraq in 2003\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef60ed0c-a391-4602-a26c-4e2a379549aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForCausalLM, GenerationConfig\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "pd.set_option('display.max_colwidth', 400)\n",
    "\n",
    "login(token=os.environ[\"HUGGINGFACE_TOKEN\"])\n",
    "\n",
    "event_names = {\"IraqWar\":\"Iraq War\", \"CrimeaCrisis\":\"Crimea Crisis\", \"CapitolRiot\":\"Capitol Riot\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e2f840-8175-44aa-a7af-1305c6c2187e",
   "metadata": {},
   "source": [
    "# Model config\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4b0a80-d063-4c08-9e97-9c6c9eb45225",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-2-70b-chat-hf\"\n",
    "torch_dtype = torch.bfloat16\n",
    "quantization = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a6abaa-2c83-42d4-868a-b8efcc55dca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc6ba7c-cf12-4954-975b-9a3607deaf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_and_tokenizer(state):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    if quantization:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=nf4_config)\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch_dtype).to(state.device)\n",
    "    return model, tokenizer\n",
    "\n",
    "def tokenize_sample(sample, model, tokenizer):\n",
    "    if isinstance(sample, str):\n",
    "        inputs = tokenizer(sample, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    else:\n",
    "        inputs = tokenizer.apply_chat_template(sample, return_tensors=\"pt\")\n",
    "    \n",
    "    inputs = inputs.to(model.device)\n",
    "    return inputs\n",
    "    \n",
    "def generate_single_token(batch, model, tokenizer):\n",
    "    inputs = tokenize_sample(batch[0], model, tokenizer)\n",
    "    out = model.generate(inputs, pad_token_id=tokenizer.eos_token_id, max_new_tokens=1)\n",
    "    r = tokenizer.decode(out[0, -1])\n",
    "    return [r]\n",
    "\n",
    "def generate_full(batch, model, tokenizer):\n",
    "    inputs = tokenize_sample(batch[0], model, tokenizer)\n",
    "    out = model.generate(inputs, pad_token_id=tokenizer.eos_token_id, max_new_tokens=1000)\n",
    "    r = tokenizer.decode(out[0, inputs.shape[1]:], skip_special_tokens=True)\n",
    "    return [r]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defcd45f-0079-4b7c-992b-25e7f96406fa",
   "metadata": {},
   "source": [
    "# Step 0: Prepare Distributed Inference\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ed462b-0169-4b71-aef2-b2f0314d889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributed_inference import InferenceContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd343c3-53fd-4304-bf30-2ea4368a0e54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ic = InferenceContext()\n",
    "ic.start(prepare_model_and_tokenizer, generate_single_token, num_processes=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52456db-eb12-4c01-9323-1e0618dee9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer after forking main process, hf doesnt like it the other way around\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8018dbd3-27e2-4cc0-abe1-ba9a36c900b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare sentence embedding model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94822221-c59d-4b28-b3f8-57c98fc8bd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import urllib\n",
    "import time\n",
    "from os.path import join\n",
    "\n",
    "def extract_level(viewpoint, subevent=None):\n",
    "    main_event = \"Iraq War\"\n",
    "\n",
    "    if subevent is not None:\n",
    "        event = subevent + \" during the \" + main_event\n",
    "    else:\n",
    "        event = main_event\n",
    "    print(event)\n",
    "    \n",
    "    folder = \"newspaper_articles/\"\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    d = []\n",
    "    for x in os.listdir(join(folder, viewpoint)):\n",
    "        if x.startswith(\".\"): continue\n",
    "        print(folder, x)\n",
    "        v = pd.read_json(join(folder, viewpoint, x)).T\n",
    "        v[\"viewpoint\"] = x.split(\"=\")[2].split(\"-\")[0]\n",
    "        v[\"source\"] = v.index.map(lambda x: urllib.parse.urlparse(x).hostname)\n",
    "\n",
    "        d.append(v)\n",
    "\n",
    "    v = pd.concat(d)\n",
    "    data[\"Iraq War\"] = v\n",
    "    \n",
    "    print(\"Loaded articles:\")\n",
    "    for k,v in data.items():\n",
    "        print(f\"\\t{k}: {len(v)}\")\n",
    "        \n",
    "    for k,v in data.items():\n",
    "        data[k] = v[v[\"text\"].apply(len)<8000]\n",
    "\n",
    "    print(\"Filtered out lengthy articles. Remaining:\")\n",
    "    for k,v in data.items():\n",
    "        print(f\"{k}: {len(v)}\")\n",
    "        g = v.groupby(\"viewpoint\").apply(len)\n",
    "        for i,j in g.to_dict().items():\n",
    "            print(f\"\\t{i}: {j}\")\n",
    "\n",
    "    for k,v in data.items():\n",
    "        data[k] = v[v[\"text\"].apply(len)>1000]\n",
    "\n",
    "    print(\"Filtered out short articles. Remaining:\")\n",
    "    for k,v in data.items():\n",
    "        print(f\"{k}: {len(v)}\")\n",
    "        g = v.groupby(\"viewpoint\").apply(len)\n",
    "        for i,j in g.to_dict().items():\n",
    "            print(f\"\\t{i}: {j}\")\n",
    "            \n",
    "    def get_conflict_filter_conversation(event, article):\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": f\"Here is a news article. Is it about the {event}? '{article}'\\n Answer only with yes or no only if the article is clearly about the {event} (2003-2011).\"},\n",
    "        ]    \n",
    "        s = tokenizer.apply_chat_template(messages, tokenize=False).rstrip() + (\" \" if \"llama\" in model_name else \"\")\n",
    "        return s\n",
    "    \n",
    "    articles = data[main_event]\n",
    "    articles = articles[articles[\"viewpoint\"] == viewpoint]\n",
    "\n",
    "    prompts = [get_conflict_filter_conversation(event, a) for a in articles[\"text\"]]\n",
    "    \n",
    "    ic.set_on_batch_received(generate_single_token)\n",
    "    time.sleep(1)\n",
    "    answers = ic.run_inference(prompts, max_batch_size=1)    \n",
    "    \n",
    "    # these are all articles about e:\n",
    "    d_e = articles[[x == \"Yes\" for x in answers]].copy()\n",
    "    print(\"Relevant articles\", len(d_e))\n",
    "    if len(d_e) == 0:\n",
    "        print(\"No more source to cover!\")\n",
    "        return\n",
    "\n",
    "    def get_event_skeletion_extraction_conversation(event, article, sys_prompt=None):\n",
    "        input_text = f\"Here is a news article: '{article}'\\nList the major events of the {event} in chronological order as reported in the article. Keep it consise and remove everything unrelated.\"\n",
    "        messages = []\n",
    "        if sys_prompt is not None:\n",
    "            messages = [{\"role\":\"system\", \"content\":sys_prompt}]\n",
    "\n",
    "        messages += [\n",
    "            {\"role\": \"user\", \"content\": input_text},\n",
    "        ]\n",
    "\n",
    "        s = tokenizer.apply_chat_template(messages, tokenize=False).rstrip()\n",
    "        s += f\" Here is a numbered list of the major events of the {event} in chronological order as reported in the article:\\n\\n\"\n",
    "        return s\n",
    "\n",
    "    prompts = [get_event_skeletion_extraction_conversation(event, a, sys_prompt=None) for a in d_e[\"text\"]]\n",
    "    # full answers should be generated now\n",
    "    ic.set_on_batch_received(generate_full)\n",
    "    time.sleep(1)\n",
    "    answers = ic.run_inference(prompts, max_batch_size=1)\n",
    "    \n",
    "    # we'll safe the original model output as event skeletons, and extract the sub events as a list under \"timeline\"\n",
    "    d_e[\"event_skeleton\"] = answers\n",
    "    \n",
    "    import re\n",
    "\n",
    "    def extract_timeline(r):\n",
    "        pattern = r\"^\\d.*?$\"\n",
    "        matches = re.findall(pattern, r, re.DOTALL | re.MULTILINE)\n",
    "        '''\n",
    "        t = []\n",
    "        for line in r.split(\"\\n\"):\n",
    "            try:\n",
    "                _, line_nr, text = re.split(\"(\\d+)\\.* \", line, maxsplit=1)\n",
    "                t.append(text)\n",
    "            except:\n",
    "                print(\"Failed here:\")\n",
    "                print(f\"'{line}'\")\n",
    "                print(r)\n",
    "        '''\n",
    "        # filter out duplicates\n",
    "        r = {}\n",
    "        for x in matches:\n",
    "            if x in r: continue\n",
    "            r[x] = None\n",
    "        t = list(r.keys())\n",
    "        return t\n",
    "    \n",
    "    d_e[\"timeline\"] = d_e[\"event_skeleton\"].apply(extract_timeline)\n",
    "\n",
    "    # filter out short or empty timelines\n",
    "    d_e = d_e[d_e[\"timeline\"].apply(len) > 0]\n",
    "    print(len(d_e),\"articles,\", d_e[\"timeline\"].apply(len).sum(), \"subevents\")\n",
    "\n",
    "    if len(d_e) == 1:\n",
    "        print(\"Nothing else to be done! This is the lowest you can go right now:\\n\")\n",
    "        for p in d_e[\"timeline\"][0]:\n",
    "            print(p)\n",
    "        return\n",
    "\n",
    "    def unfold_timeline(row):\n",
    "        r = row.to_frame().T\n",
    "        c = pd.concat([r]*len(row[\"timeline\"]))\n",
    "        c[\"timeline\"] = row[\"timeline\"]\n",
    "        return c\n",
    "\n",
    "    unfolded = d_e.apply(unfold_timeline, axis=1)\n",
    "    unfolded = pd.concat(unfolded.tolist())\n",
    "\n",
    "    unfolded = unfolded.rename({\"timeline\":\"subevent\"}, axis=1)\n",
    "    unfolded = unfolded.reset_index(names=\"url\")\n",
    "    \n",
    "    examples = {\n",
    "        \"4. 5 November 2006: The court finds Hussein guilty on the charges and sentences him to death by hanging.\":\"Hussein sentenced to death (November 2006).\",\n",
    "        \"5. 2003-2018: Thousands of American soldiers returned home with PTSD\":\"Return of American soldiers (2003-2018).\",\n",
    "        \"1. Less than a month into the invasion - Baghdad fell to the US-led coalition forces.\":\"US-led coalition conquers Baghad (not specified).\",\n",
    "        \"9. 2003: US-led forces used depleted uranium weapons in civilian-populated areas during the military campaign.\":\"US-led forces use depleted uranium weapons in populated areas (2003).\",\n",
    "        \"2. September 11, 2001 - The terrorist attacks on the World Trade Center in New York City that led to the US invasion of Iraq.\":\"9/11 attacks (2001).\",\n",
    "        \"2. 2003: Then-US Secretary of State Colin Powell presented alleged evidence of Iraqi weapons of mass destruction, including biological ones, at the United Nations Security Council.\":\"Colin Powell presents false WMD evidence at UN (2003).\",\n",
    "        \"4. Radical Islamist group Jemaah Islamiyah convicted of crimes related to the 2002 Bali attack\": \"Conviction of islamist group Jemaah Islamiyah (not specified).\",\n",
    "        \"5. 2014-2016 - The US responded to the rise of Islamic State (IS, formerly ISIS) with airstrikes, resulting in the deaths of more than 3,000 civilians.\":\"Rise of the Islamic State (IS) (2014-2016).\"\n",
    "    }\n",
    "\n",
    "    def get_subevent_naming_conversation(se):\n",
    "        def turn(e,a):\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": f\"I need a precise headline for this event description: '{e}'.\"},\n",
    "                {\"role\": \"assistant\", \"content\": f\"{a}\"}\n",
    "            ]\n",
    "            return messages\n",
    "        turns = sum([turn(k,v) for k,v in examples.items()], []) + turn(se, \"\")\n",
    "        turns = turns[:-1]\n",
    "\n",
    "        s = tokenizer.apply_chat_template(turns, tokenize=False).rstrip() + (\" \" if \"llama\" in model_name else \"\")\n",
    "        return s\n",
    "\n",
    "    prompts = [get_subevent_naming_conversation(x) for x in unfolded[\"subevent\"]]\n",
    "\n",
    "    ic.set_on_batch_received(generate_full)\n",
    "    time.sleep(1)\n",
    "    answers = ic.run_inference(prompts)\n",
    "\n",
    "    unfolded[\"subevent_short\"] = answers\n",
    "    \n",
    "    time_samples = {\n",
    "        \"US and Britain launch Iraq War (2003).\": \"Yes\",\n",
    "        \"Saddam Hussein leads Sunni dictatorship in Iraq, goes to war with Iran (1980s).\":\"No\",\n",
    "        \"Battle of Fallujah (2003).\":\"Yes\",\n",
    "        \"James Mattis leads US Central Command (2010-2013).\":\"Yes\",\n",
    "        \"654,965 excess deaths estimated in Iraq war (2003-2006).\":\"Yes\",\n",
    "        \"Water Shortages Affect Locals\":\"No\",\n",
    "        \"Bilderberg Group discusses Iraq war (2002).\":\"Yes\",\n",
    "        \"US invests heavily in Iraq's economy (2003-2014).\":\"Yes\",\n",
    "        \"US airstrikes kill over 3,000 civilians in response to Islamic State (IS) rise (2014-2016)\":\"No\",\n",
    "        \"US-led coalition fights bloody battles for Fallujah (2004).\":\"Yes\",\n",
    "        \"2018 IAEA inspection finds no evidence of nuclear weapons development at Iran's Turquzabad site.\":\"No\",\n",
    "        \"US struggles to compete with China's Belt and Road initiative (not specified).\":\"No\",\n",
    "        \"US begins air strikes (March 19, 2003).\":\"Yes\",\n",
    "        \"Denmark's PM supports US invasion of Iraq (2002).\":\"Yes\",\n",
    "        \"Poverty in Iraq persists (not specified).\":\"No\",\n",
    "        \"Sunni-Shiite and ethnic tensions escalate (not specified).\":\"No\",\n",
    "        \"Arab Spring influenced by Iraq War protests (2003).\":\"Yes\",\n",
    "        \"Margaret Aldred chairs Iraq senior officials group (2002).\":\"Yes\",    \n",
    "    }\n",
    "\n",
    "    def create_few_shot_convos(examples, prompt, last_sample):\n",
    "        def turn(e,answer):\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": prompt.format(example=e)},\n",
    "                {\"role\": \"assistant\", \"content\": f\"{answer}\"}\n",
    "            ]\n",
    "            return messages\n",
    "        turns = sum([turn(k,v) for k,v in examples.items()], []) + turn(last_sample,\"\")\n",
    "        turns = turns[:-1]\n",
    "        return turns\n",
    "\n",
    "    def get_subevent_filter_time_conversation(se):\n",
    "        prompt = \"Did '{example}' happen between 2000 and 2012?\"\n",
    "        messages = create_few_shot_convos(time_samples, prompt, se)\n",
    "        s = tokenizer.apply_chat_template(messages, tokenize=False).rstrip() + (\" \" if \"llama\" in model_name else \"\")\n",
    "        return s\n",
    "\n",
    "    prompts = [get_subevent_filter_time_conversation(row[\"subevent_short\"]) for i, row in unfolded.iterrows()]\n",
    "\n",
    "    ic.set_on_batch_received(generate_single_token)\n",
    "    answers = ic.run_inference(prompts, max_batch_size=1)\n",
    "\n",
    "    print(np.unique(answers, return_counts=True))\n",
    "\n",
    "    #unfolded[[x == \"No\" for x in answers]][[\"subevent_short\",\"subevent\"]].sample(10)\n",
    "\n",
    "    if subevent is None:\n",
    "        unfolded = unfolded[[x == \"Yes\" for x in answers]]\n",
    "        print(\"After removing events that happened before or after 2000-2012, we retain\",len(unfolded),\"subevents\")\n",
    "    else:\n",
    "        print(\"Time filtering not active, due to recursion step\")\n",
    "\n",
    "    unfolded = unfolded.groupby(\"url\").filter(lambda x: len(x) > 1)\n",
    "    print(\"After removing small document graphs:\",len(unfolded), \"subevents\")\n",
    "    \n",
    "    embeddings = model.encode(unfolded[\"subevent_short\"].to_list(), show_progress_bar=True)\n",
    "\n",
    "    from sklearn.metrics.pairwise import cosine_distances, euclidean_distances\n",
    "    dists = euclidean_distances(embeddings, embeddings)\n",
    "\n",
    "    from sklearn.cluster import DBSCAN, HDBSCAN, OPTICS\n",
    "\n",
    "    min_cluster_size = np.clip(len(unfolded)//10, 2, 5)\n",
    "\n",
    "    cluster = HDBSCAN(metric=\"precomputed\", cluster_selection_epsilon=0.1, min_cluster_size=min_cluster_size, min_samples=2, algorithm=\"auto\", cluster_selection_method=\"eom\")\n",
    "    #cluster = OPTICS(min_samples=5, metric=\"precomputed\", eps=1.0, cluster_method=\"dbscan\")\n",
    "    c = cluster.fit_predict(dists)\n",
    "\n",
    "    print(\"Clusters\", np.unique(c, return_counts=True))\n",
    "\n",
    "    clusters, counts = np.unique(c, return_counts=True)\n",
    "    matched_clusters = clusters[counts > 2]\n",
    "    components = [unfolded[\"subevent_short\"][c == x].to_list() for x in matched_clusters if x != -1]\n",
    "    if len(components) < 1:\n",
    "        print(\"Found no cluster of subevents, terminating\")\n",
    "        return\n",
    "\n",
    "    #[print(f\"{len(x)}\\n\" + str(x) + \"\\n\") for x in components]\n",
    "\n",
    "    \n",
    "    def get_subevent_canonicalization_conversation(event, se):\n",
    "        e = \"\\n\".join(se)\n",
    "        messages = [\n",
    "\n",
    "            {\n",
    "                \"role\": \"user\", \"content\": f\"Here are sentences describing a specific process or sub-event during the {event}: \"\n",
    "                 f\"\\n\\n{e}\\n\\n\"\n",
    "                f\"I need a short and concise headline that conveys the event described by the sentences. Note that \\\"{event}\\\" is too general! If possible, include the date of the event.\"\n",
    "            },\n",
    "        ]\n",
    "        s = tokenizer.apply_chat_template(messages, tokenize=False).rstrip() + \" Certainly, a precise headline would be \\\"\"\n",
    "        return s\n",
    "\n",
    "    prompts = [get_subevent_canonicalization_conversation(event, x[:10]) for x in components]\n",
    "\n",
    "    quote_id = [tokenizer.eos_token_id] +  list({k:v for k,v in tokenizer.get_vocab().items() if k.endswith(\"\\\"\")}.values())\n",
    "    def generate_until(batch, model, tokenizer):\n",
    "        inputs = tokenize_sample(batch[0], model, tokenizer)\n",
    "        out = model.generate(inputs, pad_token_id=tokenizer.eos_token_id, max_new_tokens=1000, eos_token_id=quote_id, do_sample=False)\n",
    "        r = tokenizer.decode(out[0, inputs.shape[1]:], skip_special_tokens=True)\n",
    "        return [r]\n",
    "\n",
    "    ic.set_on_batch_received(generate_until)\n",
    "\n",
    "    import networkx as nx\n",
    "\n",
    "    answers = ic.run_inference(prompts, max_batch_size=1)\n",
    "    subevent_names = [x[:x.find(\"\\\"\")] for x in answers]\n",
    "\n",
    "    sn = subevent_names.copy()\n",
    "    comps = components.copy()\n",
    "\n",
    "    # recursively merge subevents until similarities get to low\n",
    "    while True:\n",
    "        # compute embeddings\n",
    "        embeddings = model.encode(sn, show_progress_bar=False)\n",
    "        sims = 1- cosine_distances(embeddings, embeddings)\n",
    "        sims[np.diag_indices_from(sims)] = 0\n",
    "\n",
    "        # get graph from adjacencies based on similarities\n",
    "        g = nx.from_numpy_array(sims > 0.7)\n",
    "        connected = list(nx.connected_components(g))\n",
    "\n",
    "        if not any(len(x) > 1 for x in connected): break\n",
    "\n",
    "        # merge lists of subevents from one graph component together\n",
    "        comps = [sum([comps[i] for i in x], []) for x in connected]\n",
    "        merged_names = [[sn[i] for i in x] for x in connected]\n",
    "        small, large = [x[0] for x in merged_names if len(x) == 1], [x for x in merged_names if len(x) !=1]\n",
    "        # find names for large components\n",
    "        prompts = [get_subevent_canonicalization_conversation(event, x) for x in large]\n",
    "        answers = ic.run_inference(prompts, max_batch_size=1)\n",
    "        new_names = [x[:x.find(\"\\\"\")] for x in answers]\n",
    "\n",
    "\n",
    "\n",
    "        for i,j in zip(large, new_names):\n",
    "            print(f\"Derived \\\"{j}\\\" from\")\n",
    "            for l in i:\n",
    "                print(\"\\t\",l)\n",
    "        # compile new names and components\n",
    "        sn = small + new_names\n",
    "\n",
    "    subevent_names = sn\n",
    "    components = comps\n",
    "\n",
    "    major_subevents = pd.DataFrame({\"major_subevent\":subevent_names, \"subevents\":components})\n",
    "    print(\"Num major subevents:\", len(major_subevents))\n",
    "    \n",
    "    if len(major_subevents) < 2:\n",
    "        print(\"Too few subevents for event-event relation extraction\")\n",
    "        return\n",
    "    from itertools import product\n",
    "\n",
    "    (yes_id,),(no_id,) = tokenizer([\"Yes\",\"No\"], add_special_tokens=False, return_attention_mask=False).input_ids\n",
    "\n",
    "    def generate_probs(batch, model, tokenizer):\n",
    "        inputs = tokenize_sample(batch[0], model, tokenizer)\n",
    "        out = model.generate(inputs, pad_token_id=tokenizer.eos_token_id, max_new_tokens=1, output_scores=True, return_dict_in_generate=True)\n",
    "        probs = torch.softmax(out.scores[0][0].cpu(), dim=-1)\n",
    "        #r = tokenizer.decode(out[0, -1])\n",
    "        return [probs[[yes_id, no_id]].tolist()]\n",
    "\n",
    "    def get_e2e_conv(event, se1, se2, relation):\n",
    "        messages = [\n",
    "            {\"role\":\"user\", \"content\": f\"I need information regarding the {event}. Did '{se1}' {relation} '{se2}'? Answer only with yes or no.\"}\n",
    "        ]\n",
    "        s = tokenizer.apply_chat_template(messages, tokenize=False).rstrip() + \" \"\n",
    "        return s\n",
    "\n",
    "    t = [x for x in product(major_subevents[\"major_subevent\"].to_list(),major_subevents[\"major_subevent\"].to_list(), [\"happened after\"]) if x[0] != x[1]]\n",
    "    pairs = pd.DataFrame(t, columns=[\"left_major\",\"right_major\",\"relation\"])\n",
    "\n",
    "    prompts = [get_e2e_conv(event, row.left_major, row.right_major, row.relation) for _, row in pairs.iterrows()]\n",
    "\n",
    "    ic.set_on_batch_received(generate_probs)\n",
    "\n",
    "    p = ic.run_inference(prompts)\n",
    "    p = np.array(p)\n",
    "\n",
    "    pairs[\"prob_yes\"] = p[:,0]\n",
    "    pairs[\"prob_no\"] = p[:,1]\n",
    "\n",
    "    import networkx as nx\n",
    "    from IPython.display import Image\n",
    "\n",
    "    G = nx.MultiDiGraph()\n",
    "\n",
    "\n",
    "    for _, row in pairs.reset_index().iterrows():\n",
    "        if row.prob_no > 0.25: continue\n",
    "        G.add_edge(row.left_major, row.right_major, label=row.relation, yes_prob=row.prob_yes, no_prob = row.prob_no)\n",
    "\n",
    "    # for debugging\n",
    "    a = nx.drawing.nx_agraph.to_agraph(G)\n",
    "    \n",
    "    folder = \"extractions/\" + main_event + \"/\" + viewpoint + (\"/\" + subevent if subevent is not None else \"\")\n",
    "    # create folder for current event\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    # create folders for all subevents\n",
    "    for s in major_subevents[\"major_subevent\"]:\n",
    "        os.makedirs(folder + \"/\" + s.removesuffix(\".\"), exist_ok=True)\n",
    "\n",
    "    # save relations\n",
    "    pairs.to_csv(folder + \"/relations.csv\")\n",
    "\n",
    "    # save graph\n",
    "    a.draw(folder + \"/graph.png\", prog=\"dot\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac2e572-f95e-42f7-9e3d-bfb0787e6953",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_level(viewpoint=\"RUS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb51faeb-c8aa-4acd-98f1-2050f110409d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p = \"extractions/Iraq War/RUS/\"\n",
    "folders = [x for x in os.listdir(p) if os.path.isdir(p+x) and not x.startswith(\".\")]\n",
    "folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17487cfc-4c1c-44db-bbd4-006352ef29ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for f in folders:\n",
    "    extract_level(viewpoint=\"RUS\", subevent=f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
